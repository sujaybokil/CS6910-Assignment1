{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flying-luxembourg",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px; text-align:center\">Assignment 1</p>\n",
    "<p style=\"font-size:18px; text-align:center\">CS6910: Fundamentals of Deep Learning</p>\n",
    "<p>Sujay Bokil: ME17B120<br>\n",
    "Avyay Rao: ME17B130</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cathedral-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# import templates that we have created to make different kinds of layers, losses, optimizers etc.\n",
    "from sujay.templates import AutoDiffFunction, Layer, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-default",
   "metadata": {},
   "source": [
    "<b>Outline of the framework</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-nicholas",
   "metadata": {},
   "source": [
    "<b>Activation functions</b> \n",
    "\n",
    "For this assignment we implement the following activation functions:\n",
    "\n",
    "1. Sigmoid activation\n",
    "\n",
    "$$y = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{-e^{-x}}{(1 + e^{-x})^2} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "2. ReLU activation\n",
    "\n",
    "$$y = ReLU(x) = max(0, x)$$\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x\\geq 0\\\\\n",
    "      0 & x\\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.$$\n",
    "\n",
    "3. Tanh activation\n",
    "$$y = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "$$\\frac{dy}{dx} = \\frac{4}{(e^x + e^{-x})^2} = 1 - (tanh(x))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "going-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(AutoDiffFunction):\n",
    "    \"\"\" \n",
    "    Represents the Sigmoid Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = 1/(1 + np.exp(-x))\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": y*(1-y)}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]      \n",
    "\n",
    "\n",
    "class ReLU(AutoDiffFunction):\n",
    "    \"\"\" \n",
    "    Represents the RelU Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = np.where(x>0.0, 1.0, 0.0)\n",
    "\n",
    "        return x * self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        return {\"x\": self.saved_for_backward}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]\n",
    "    \n",
    "class Tanh(AutoDiffFunction):\n",
    "    \"\"\" \n",
    "    Represents the Tanh Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": 1 - y**2}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-favor",
   "metadata": {},
   "source": [
    "<b>Layers</b>\n",
    "\n",
    "For this assignment, we only use fully connected OR Dense layers where each input neuron is connected to each output neuron, along with a bias unit. Below is a representation of a fully connected layer.\n",
    "\n",
    "![Representation of a fully connected layer](FullyConnectedLayer.png)\n",
    "\n",
    "The equation for such a layer is simply\n",
    "\n",
    "$$y = FullyConnected(x) = wx + b$$\n",
    "\n",
    "$$\\frac{dy}{dw} = x^T \\quad\\frac{dy}{dx} = w^T  \\quad\\frac{dy}{db} = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tired-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Layer):\n",
    "    \"\"\"\n",
    "    Class representing a fully connected layer, the weights inside the class decide it's output\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.initialize_weights(in_dim, out_dim)\n",
    "\n",
    "    def initialize_weights(self, in_dim, out_dim):\n",
    "        \n",
    "        self.weights[\"w\"] = np.random.randn(in_dim, out_dim)\n",
    "        self.weights[\"b\"] = np.random.randn(1, out_dim)\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        \n",
    "        gradients = {}\n",
    "\n",
    "        # y = x * w + b        \n",
    "        # we compute gradients wrt w and x \n",
    "        # gradient wrt b is not required explicitly since we know that it's value is 1\n",
    "        gradients[\"w\"] = self.saved_for_backward[\"x\"].T\n",
    "        gradients[\"x\"] = self.weights[\"w\"].T\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = x @ self.weights[\"w\"] + self.weights[\"b\"]\n",
    "        self.saved_for_backward[\"x\"] = x\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, dy):\n",
    "        dx = dy @ self.grad[\"x\"]\n",
    "        \n",
    "        # calculating gradients wrt weights\n",
    "        dw = self.grad[\"w\"] @ dy\n",
    "        db = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "        self.absolute_gradients = {\"w\": dw, \"b\": db}\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.optimizer.step(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-conversation",
   "metadata": {},
   "source": [
    "<b>Loss function</b>\n",
    "\n",
    "The loss function dictates how good the output of the neural network is. Since we use MNIST dataset, our job is classification and hence we use the Categorical CrossEntropy loss function. The equation is given as\n",
    "\n",
    "\n",
    "$$L(p, y) = \\Sigma_{i=1}^{N} \\Sigma_{k=1}^{K} y_{ik} \\log p_{ik}$$ \n",
    "\n",
    "where $$y_{ik} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x \\in class-k\\\\\n",
    "      0 & else \\\\\n",
    "\\end{array} \n",
    "\\right.$$\n",
    "\n",
    "$p_{ik} =$ probability that $i^{th}$ sample falls in $k^{th}$ class\n",
    "\n",
    "In our implementation, the given loss function is applied along with the activation function for the last layer i.e. Softmax activation. It's formula is given by the following equation\n",
    "\n",
    "$ f: [x_1, x_2, ... x_k] \\rightarrow [p_1, p_2, ... p_k]$ such that $p_i = \\frac{e^{x_i}}{\\Sigma_{k=1}^{K} e^{x_i}}$\n",
    "\n",
    "Now, to find the derivative of loss w.r.t input we have apply the chain rule. Let $p(x)$ represent the softmax activation and $L$ represent the loss. Then the expression turns out to be\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L(p, y)}{\\partial p} \\frac{\\partial p(x)}{\\partial x} = p - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-weekly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
