{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\r\n",
    "import numpy as np \r\n",
    "from sklearn.datasets import make_classification\r\n",
    "\r\n",
    "from templates import AutoDiffFunction, Layer, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(AutoDiffFunction):\r\n",
    "    def __init__(self) -> None:\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        self.saved_for_backward = 1/(1 + np.exp(-x))\r\n",
    "        return self.saved_for_backward\r\n",
    "\r\n",
    "    def compute_grad(self, x):\r\n",
    "        y = self.saved_for_backward\r\n",
    "\r\n",
    "        return {\"x\": y*(1-y)}\r\n",
    "\r\n",
    "    def backward(self, dy):\r\n",
    "        return dy * self.grad[\"x\"]      \r\n",
    "\r\n",
    "\r\n",
    "class RelU(AutoDiffFunction):\r\n",
    "    def __init__(self) -> None:\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        self.saved_for_backward = np.where(x>0.0, 1.0, 0.0)\r\n",
    "\r\n",
    "        return x * self.saved_for_backward\r\n",
    "\r\n",
    "    def compute_grad(self, x):\r\n",
    "        return {\"x\": self.saved_for_backward}\r\n",
    "\r\n",
    "    def backward(self, dy):\r\n",
    "        return dy * self.grad[\"x\"]\r\n",
    "     \r\n",
    "class Softmax(AutoDiffFunction):\r\n",
    "    def __init__(self) -> None:\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        v = np.exp(x)\r\n",
    "        self.saved_for_backward = v\r\n",
    "\r\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\r\n",
    "\r\n",
    "    def compute_grad(self, x):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def backward(self, dy):\r\n",
    "        return dy * self.grad[\"x\"]\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Layer):\r\n",
    "    def __init__(self, in_dim, out_dim, weight_decay=None, init_method=\"random\") -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.weight_decay = weight_decay\r\n",
    "        self.init_method = init_method\r\n",
    "        self.initialize_weights(in_dim, out_dim)\r\n",
    "\r\n",
    "    def initialize_weights(self, in_dim, out_dim):\r\n",
    "        \r\n",
    "        if self.init_method == \"random\":\r\n",
    "            scaling_factor = 1/np.sqrt(in_dim)\r\n",
    "            self.weights[\"w\"] = np.random.randn(in_dim, out_dim) * scaling_factor\r\n",
    "            self.weights[\"b\"] = np.random.randn(1, out_dim) * scaling_factor\r\n",
    "        elif self.init_method == \"xavier\":\r\n",
    "            lim = np.sqrt(6 / (in_dim + out_dim))\r\n",
    "            self.weights[\"w\"] = np.random.uniform(low=-lim, high=lim, size=(in_dim, out_dim))\r\n",
    "            self.weights[\"b\"] = np.random.uniform(low=-lim, high=lim, size=(1, out_dim))\r\n",
    "\r\n",
    "    def compute_grad(self, x):\r\n",
    "        \r\n",
    "        gradients = {}\r\n",
    "\r\n",
    "        # y = x * w + b        \r\n",
    "        # we compute gradients wrt w and x \r\n",
    "        # gradient wrt b is not required explicitly since we know that it's value is 1\r\n",
    "        gradients[\"w\"] = self.saved_for_backward[\"x\"].T\r\n",
    "        gradients[\"x\"] = self.weights[\"w\"].T\r\n",
    "\r\n",
    "        return gradients\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        output = x @ self.weights[\"w\"] + self.weights[\"b\"]\r\n",
    "        self.saved_for_backward[\"x\"] = x\r\n",
    "        \r\n",
    "        return output\r\n",
    "\r\n",
    "    def backward(self, dy):\r\n",
    "        \r\n",
    "        # calculating gradients wrt input to pass on to previous layer for backprop\r\n",
    "        dx = dy @ self.grad[\"x\"]\r\n",
    "        \r\n",
    "        # calculating gradients wrt weights\r\n",
    "        dw = self.grad[\"w\"] @ dy\r\n",
    "        db = np.sum(dy, axis=0, keepdims=True)\r\n",
    "\r\n",
    "        # accomodating for weight_decay / regularization\r\n",
    "        if self.weight_decay:\r\n",
    "            dw = dw + 2 * self.weight_decay * self.weights[\"w\"]\r\n",
    "            db = db + 2 * self.weight_decay * self.weights[\"b\"]\r\n",
    "\r\n",
    "        self.absolute_gradients = {\"w\": dw, \"b\": db}\r\n",
    "\r\n",
    "        return dx\r\n",
    "\r\n",
    "    def update_weights(self):\r\n",
    "        self.optimizer.step(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the loss function\r\n",
    "\r\n",
    "### For this particular problem, we require CrossEntropy Loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFromLogits(Loss):\r\n",
    "    def __init__(self) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.n_classes = None\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def softmax(x):\r\n",
    "        v = np.exp(x)\r\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\r\n",
    "\r\n",
    "    def encode(self, y): \r\n",
    "        encoded_y = np.zeros(shape=(len(y), self.n_classes))\r\n",
    "\r\n",
    "        for i in range(len(y)):\r\n",
    "            encoded_y[i,y[i]] = 1\r\n",
    "\r\n",
    "        return encoded_y\r\n",
    "\r\n",
    "    def forward(self, y_pred, y_true):\r\n",
    "         \r\n",
    "        probabilities = self.softmax(y_pred)\r\n",
    "        y_true_encoded = self.encode(y_true)\r\n",
    "\r\n",
    "        loss_value = np.mean(np.sum(- y_true_encoded * np.log(probabilities), axis=1))\r\n",
    "\r\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\r\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\r\n",
    "\r\n",
    "        return loss_value\r\n",
    "\r\n",
    "    def compute_grad(self, y_pred, y_true):\r\n",
    "\r\n",
    "        return {\"x\": self.saved_for_backward[\"probabilities\"] - self.saved_for_backward[\"y_true\"]}        \r\n",
    "\r\n",
    "\r\n",
    "class MSELossFromLogits(Loss):\r\n",
    "    def __init__(self) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.n_classes = None\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def softmax(x):\r\n",
    "        v = np.exp(x)\r\n",
    "\r\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\r\n",
    "\r\n",
    "    def encode(self, y): \r\n",
    "        encoded_y = np.zeros(shape=(len(y), self.n_classes))\r\n",
    "\r\n",
    "        for i in range(len(y)):\r\n",
    "            encoded_y[i,y[i]] = 1\r\n",
    "\r\n",
    "        return encoded_y\r\n",
    "    \r\n",
    "    @staticmethod\r\n",
    "    def indicator(i, j):\r\n",
    "        ind = {True: 1, False: 0}\r\n",
    "        return ind[i==j]\r\n",
    "\r\n",
    "    def forward(self, y_pred, y_true):\r\n",
    "         \r\n",
    "        probabilities = self.softmax(y_pred)\r\n",
    "        y_true_encoded = self.encode(y_true)\r\n",
    "\r\n",
    "        loss_value = np.mean(np.sum((probabilities - y_true_encoded)**2, axis=1))\r\n",
    "\r\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\r\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\r\n",
    "\r\n",
    "        return loss_value\r\n",
    "\r\n",
    "    def compute_grad(self, y_pred, y_true):\r\n",
    "\r\n",
    "        probs = self.saved_for_backward[\"probabilities\"]\r\n",
    "        labels = self.saved_for_backward[\"y_true\"]\r\n",
    "        grad = np.zeros(shape=(len(y_true), self.n_classes))\r\n",
    "        \r\n",
    "        for point_counter in range(len(y_true)):\r\n",
    "            res = 0\r\n",
    "            for i in range(self.n_classes):\r\n",
    "                for j in range(self.n_classes):\r\n",
    "                    \r\n",
    "                    res = probs[point_counter, j] * (probs[point_counter, j] - labels[point_counter, j]) * (self.indicator(i,j) - probs[point_counter, i])\r\n",
    "                \r\n",
    "                grad[point_counter, i] = res\r\n",
    "        \r\n",
    "        return {\"x\": grad}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an optimizer for the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\r\n",
    "    def __init__(self, lr=1e-3):\r\n",
    "        super().__init__()\r\n",
    "        self.lr = lr\r\n",
    "\r\n",
    "    def step(self, layer):\r\n",
    "\r\n",
    "        for weight_name, _ in layer.weights.items():\r\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - self.lr * layer.absolute_gradients[weight_name]\r\n",
    "\r\n",
    "class Nadam(Optimizer):\r\n",
    "    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\r\n",
    "        super().__init__()\r\n",
    "        self.lr = lr\r\n",
    "        self.beta_1 = beta_1\r\n",
    "        self.beta_2 = beta_2\r\n",
    "        self.epsilon = epsilon\r\n",
    "        self.t = 1\r\n",
    "\r\n",
    "    def step(self, layer):\r\n",
    "        \r\n",
    "        # we have 2 parameters to remember m(t) and v(t) for all weights in the layer\r\n",
    "        if self.remember == {}:\r\n",
    "            for weight_name, weight in layer.weights.items():\r\n",
    "                self.remember[weight_name] = {}\r\n",
    "                self.remember[weight_name][\"v\"] = np.zeros_like(weight)\r\n",
    "                self.remember[weight_name][\"m\"] = np.zeros_like(weight)\r\n",
    "\r\n",
    "        for weight_name, weight in layer.weights.items():\r\n",
    "            \r\n",
    "            self.remember[weight_name][\"m\"] = self.beta_1 * self.remember[weight_name][\"m\"] + \\\r\n",
    "                                                (1 -self.beta_1) * layer.absolute_gradients[weight_name]\r\n",
    "\r\n",
    "            self.remember[weight_name][\"v\"] = self.beta_2 * self.remember[weight_name][\"v\"] + \\\r\n",
    "                                                (1 - self.beta_2) * layer.absolute_gradients[weight_name]**2\r\n",
    "\r\n",
    "            # bias correction step \r\n",
    "            m_hat = self.remember[weight_name][\"m\"]/(1 - self.beta_1 ** self.t)\r\n",
    "            v_hat = self.remember[weight_name][\"v\"]/(1 - self.beta_2 ** self.t)\r\n",
    "\r\n",
    "            d = self.lr / (np.sqrt(v_hat) + self.epsilon) * (self.beta_1*m_hat + (1-self.beta_1)/\r\n",
    "                                                (1-self.beta_1 ** self.t) * layer.absolute_gradients[weight_name]) \r\n",
    "\r\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - d\r\n",
    "\r\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the structure for an actual neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\r\n",
    "    def __init__(self, layers) -> None:\r\n",
    "        self.layers = layers\r\n",
    "        self.history = []\r\n",
    "\r\n",
    "    def __call__(self, *args, **kwds):\r\n",
    "        return self.forward(*args, **kwds)\r\n",
    "\r\n",
    "    def compile(self, loss, optimizer):\r\n",
    "        self.loss = loss\r\n",
    "\r\n",
    "        for layer in self.layers:\r\n",
    "            if isinstance(layer, Layer):\r\n",
    "                layer.optimizer = deepcopy(optimizer)\r\n",
    "\r\n",
    "    def calculate_loss(self, y_pred, y_true):\r\n",
    "        return self.loss(y_pred, y_true)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        for layer in self.layers:\r\n",
    "            x = layer(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "    def backward(self):\r\n",
    "\r\n",
    "        gradient = self.loss.backward()\r\n",
    "        for layer in reversed(self.layers):\r\n",
    "            gradient = layer.backward(gradient)\r\n",
    "\r\n",
    "        return gradient\r\n",
    "\r\n",
    "    def update_weights(self):\r\n",
    "\r\n",
    "        for layer in reversed(self.layers):\r\n",
    "            if isinstance(layer, Layer):\r\n",
    "                layer.update_weights()\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def accuracy_score(y_pred, y_true):\r\n",
    "\r\n",
    "        pred_labels = np.argmax(y_pred, axis=1)\r\n",
    "        return np.sum(pred_labels == y_true) / len(y_true)\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def create_batches(X, y, batch_size=32):\r\n",
    "        batches = []\r\n",
    "\r\n",
    "        for i in range(len(y) // batch_size):\r\n",
    "            start_idx = batch_size * i\r\n",
    "            end_idx = batch_size * (i + 1)\r\n",
    "\r\n",
    "            batches.append([X[start_idx: end_idx], y[start_idx: end_idx]])\r\n",
    "\r\n",
    "        # take care of the last batch which might have batch_size less than the specified one\r\n",
    "        if len(y) % batch_size != 0:\r\n",
    "            batches.append([X[end_idx:], y[end_idx:]])\r\n",
    "\r\n",
    "        return batches\r\n",
    "\r\n",
    "\r\n",
    "    def fit(self, X, y, batch_size=32, epochs=10):\r\n",
    "\r\n",
    "        # calculate number of classes to pass to the loss function\r\n",
    "        self.loss.n_classes = len(np.unique(y))\r\n",
    "\r\n",
    "        batches = self.create_batches(X, y, batch_size=batch_size)\r\n",
    "        num_batches = len(batches)\r\n",
    "\r\n",
    "        for epoch in range(1, epochs+1):\r\n",
    "\r\n",
    "            total_loss = 0\r\n",
    "            total_accuracy = 0\r\n",
    "\r\n",
    "            for X, y in batches:\r\n",
    "\r\n",
    "                preds = self(X)\r\n",
    "                total_loss += self.loss(preds, y)\r\n",
    "                total_accuracy += self.accuracy_score(preds, y)\r\n",
    "\r\n",
    "                _ = self.backward()\r\n",
    "                self.update_weights()\r\n",
    "\r\n",
    "            loss_per_epoch = total_loss / num_batches\r\n",
    "            accuracy = total_accuracy / num_batches\r\n",
    "\r\n",
    "            print(f\"Epoch: {epoch} Train Loss: {loss_per_epoch} Train Accuracy: {accuracy}\")\r\n",
    "\r\n",
    "            self.history.append({\"Epoch\" : epoch, \r\n",
    "                                    \"Train Loss\": loss_per_epoch,\r\n",
    "                                    \"Train Accuracy\": accuracy})\r\n",
    "\r\n",
    "        print(\"\\nModel trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom classification dataset to test out the function <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.844315784639989 Train Accuracy: 0.2552083333333333\n",
      "Epoch: 2 Train Loss: 0.822198569469644 Train Accuracy: 0.3020833333333333\n",
      "Epoch: 3 Train Loss: 0.8077986334843196 Train Accuracy: 0.3489583333333333\n",
      "Epoch: 4 Train Loss: 0.7947670769701066 Train Accuracy: 0.375\n",
      "Epoch: 5 Train Loss: 0.782619337862834 Train Accuracy: 0.390625\n",
      "Epoch: 6 Train Loss: 0.7715931250139082 Train Accuracy: 0.40625\n",
      "Epoch: 7 Train Loss: 0.761790289533458 Train Accuracy: 0.421875\n",
      "Epoch: 8 Train Loss: 0.7530796535002624 Train Accuracy: 0.4375\n",
      "Epoch: 9 Train Loss: 0.745543100535019 Train Accuracy: 0.453125\n",
      "Epoch: 10 Train Loss: 0.7391375678212309 Train Accuracy: 0.4635416666666667\n",
      "Epoch: 11 Train Loss: 0.7335748308563086 Train Accuracy: 0.46875\n",
      "Epoch: 12 Train Loss: 0.728398187260407 Train Accuracy: 0.484375\n",
      "Epoch: 13 Train Loss: 0.7236356215186907 Train Accuracy: 0.4895833333333333\n",
      "Epoch: 14 Train Loss: 0.7193538486350352 Train Accuracy: 0.4947916666666667\n",
      "Epoch: 15 Train Loss: 0.7155019388446241 Train Accuracy: 0.4895833333333333\n",
      "Epoch: 16 Train Loss: 0.7120023860498419 Train Accuracy: 0.5\n",
      "Epoch: 17 Train Loss: 0.7089828817756126 Train Accuracy: 0.5\n",
      "Epoch: 18 Train Loss: 0.7063449858489328 Train Accuracy: 0.5\n",
      "Epoch: 19 Train Loss: 0.7041010176092319 Train Accuracy: 0.5104166666666666\n",
      "Epoch: 20 Train Loss: 0.7023727962860046 Train Accuracy: 0.515625\n",
      "Epoch: 21 Train Loss: 0.7012080532585766 Train Accuracy: 0.515625\n",
      "Epoch: 22 Train Loss: 0.7005258727911223 Train Accuracy: 0.5260416666666666\n",
      "Epoch: 23 Train Loss: 0.700191041830851 Train Accuracy: 0.53125\n",
      "Epoch: 24 Train Loss: 0.700140089085543 Train Accuracy: 0.53125\n",
      "Epoch: 25 Train Loss: 0.7002301797434262 Train Accuracy: 0.5364583333333334\n",
      "Epoch: 26 Train Loss: 0.7003861197765464 Train Accuracy: 0.546875\n",
      "Epoch: 27 Train Loss: 0.7008388304516101 Train Accuracy: 0.546875\n",
      "Epoch: 28 Train Loss: 0.7015426995412467 Train Accuracy: 0.5416666666666666\n",
      "Epoch: 29 Train Loss: 0.702352803875038 Train Accuracy: 0.5416666666666666\n",
      "Epoch: 30 Train Loss: 0.7030518490255099 Train Accuracy: 0.5416666666666666\n",
      "Epoch: 31 Train Loss: 0.7037932797752754 Train Accuracy: 0.5416666666666666\n",
      "Epoch: 32 Train Loss: 0.7045614239965183 Train Accuracy: 0.5416666666666666\n",
      "Epoch: 33 Train Loss: 0.7053268878005645 Train Accuracy: 0.5416666666666666\n",
      "Epoch: 34 Train Loss: 0.7061768955941637 Train Accuracy: 0.5520833333333334\n",
      "Epoch: 35 Train Loss: 0.7068897970714166 Train Accuracy: 0.5520833333333334\n",
      "Epoch: 36 Train Loss: 0.7076390513140552 Train Accuracy: 0.5520833333333334\n",
      "Epoch: 37 Train Loss: 0.7084084467318806 Train Accuracy: 0.5520833333333334\n",
      "Epoch: 38 Train Loss: 0.7090750384075727 Train Accuracy: 0.5520833333333334\n",
      "Epoch: 39 Train Loss: 0.7097184230064834 Train Accuracy: 0.5572916666666666\n",
      "Epoch: 40 Train Loss: 0.7103536109144725 Train Accuracy: 0.5572916666666666\n",
      "Epoch: 41 Train Loss: 0.7110322372003814 Train Accuracy: 0.5572916666666666\n",
      "Epoch: 42 Train Loss: 0.711658678760292 Train Accuracy: 0.5572916666666666\n",
      "Epoch: 43 Train Loss: 0.7122535288960976 Train Accuracy: 0.5625\n",
      "Epoch: 44 Train Loss: 0.7129486119650256 Train Accuracy: 0.5677083333333334\n",
      "Epoch: 45 Train Loss: 0.7136767508893213 Train Accuracy: 0.5677083333333334\n",
      "Epoch: 46 Train Loss: 0.7143708938850782 Train Accuracy: 0.5677083333333334\n",
      "Epoch: 47 Train Loss: 0.7150489283235251 Train Accuracy: 0.5677083333333334\n",
      "Epoch: 48 Train Loss: 0.7158240232414957 Train Accuracy: 0.5677083333333334\n",
      "Epoch: 49 Train Loss: 0.7165090567839046 Train Accuracy: 0.5677083333333334\n",
      "Epoch: 50 Train Loss: 0.7171560984877505 Train Accuracy: 0.5625\n",
      "\n",
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "## creating a dummy dataset to test out stuff ##\r\n",
    "\r\n",
    "X, y = make_classification(n_samples=32*6, n_features=20, n_informative=15, n_classes=3)\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "scaler = StandardScaler()\r\n",
    "X = scaler.fit_transform(X)\r\n",
    "\r\n",
    "# Initializing the model and setting up loss and optimizer\r\n",
    "model = NeuralNet([FC(20, 32), RelU(), FC(32, 3)])\r\n",
    "optimizer = Nadam()\r\n",
    "loss = MSELossFromLogits()\r\n",
    "\r\n",
    "model.compile(loss, optimizer)\r\n",
    "model.fit(X, y, batch_size=32, epochs=50)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Utility functions ##\r\n",
    "def probs_to_labels(y): \r\n",
    "    return np.argmax(y, axis=1)\r\n",
    "\r\n",
    "\r\n",
    "def encoded_to_labels(y):\r\n",
    "    return np.where(y==1)[1]\r\n",
    "\r\n",
    "def accuracy_score(y_pred, y_true):\r\n",
    "\r\n",
    "    pred_labels = probs_to_labels(y_pred)\r\n",
    "\r\n",
    "    return np.sum(pred_labels == y_true) / len(y_true)\r\n",
    "\r\n",
    "def create_batches(X, y, batch_size=32):\r\n",
    "    batches = []\r\n",
    "\r\n",
    "    for i in range(len(y) // batch_size):\r\n",
    "        start_idx = batch_size * i\r\n",
    "        end_idx = batch_size * (i + 1)\r\n",
    "\r\n",
    "        batches.append([X[start_idx: end_idx], y[start_idx: end_idx]])\r\n",
    "\r\n",
    "    # take care of the last batch which might have batch_size less than the specified one\r\n",
    "    if len(y) % batch_size != 0:\r\n",
    "        batches.append([X[end_idx:], y[end_idx:]])\r\n",
    "\r\n",
    "    return batches\r\n",
    "\r\n",
    "batches = create_batches(X, y, batch_size=32)\r\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, batches, loss, optimizer, epochs=10):\r\n",
    "\r\n",
    "    training_stats = []\r\n",
    "    num_batches = len(batches)\r\n",
    "    \r\n",
    "    model.compile(loss=loss, optimizer=optimizer)\r\n",
    "\r\n",
    "    for epoch in range(1, epochs+1):\r\n",
    "\r\n",
    "        total_loss = 0\r\n",
    "        total_accuracy = 0\r\n",
    "\r\n",
    "        for X, y in batches:\r\n",
    "\r\n",
    "            preds = model(X)\r\n",
    "            total_loss += model.loss(preds, y)\r\n",
    "            total_accuracy += accuracy_score(preds, y)\r\n",
    "\r\n",
    "            _ = model.backward()\r\n",
    "            model.update_weights()\r\n",
    "\r\n",
    "        loss_per_epoch = total_loss / num_batches\r\n",
    "        accuracy = total_accuracy / num_batches\r\n",
    "\r\n",
    "        print(f\"Epoch: {epoch} Train Loss: {loss_per_epoch} Train Accuracy: {accuracy}\")\r\n",
    "\r\n",
    "        training_stats.append({\"Epoch\" : epoch, \r\n",
    "                                \"Train Loss\": loss_per_epoch,\r\n",
    "                                \"Train Accuracy\": accuracy})\r\n",
    "\r\n",
    "    \r\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-28dde0c6a003>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLossFromLogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtraining_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-17076c5d7864>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(model, batches, loss, optimizer, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mtotal_accuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\acads\\dl\\Assignment1\\sujay\\templates.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-82c24fa14af0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, y_pred, y_true)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0my_true_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0my_true_encoded\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-82c24fa14af0>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mencoded_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Initializing the model and setting up loss and optimizer\r\n",
    "model = NeuralNet([FC(20, 32, 1e-3), RelU(), FC(32, 3, 1e-3)])\r\n",
    "optimizer = SGD(lr = 0.001)\r\n",
    "loss = CrossEntropyLossFromLogits()\r\n",
    "\r\n",
    "training_stats = fit_model(model, batches, loss, optimizer, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}