{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "egyptian-piece",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px; text-align:center\">Assignment 1</p>\n",
    "<p style=\"font-size:18px; text-align:center\">CS6910: Fundamentals of Deep Learning</p>\n",
    "<p>Sujay Bokil: ME17B120<br>\n",
    "Avyay Rao: ME17B130</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closed-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# import templates that we have created to make different kinds of layers, losses, optimizers etc.\n",
    "from sujay.templates import AutoDiffFunction, Layer, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-tunisia",
   "metadata": {},
   "source": [
    "## Outline of the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-charles",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "For this assignment we implement the following activation functions:\n",
    "\n",
    "1. Sigmoid activation\n",
    "\n",
    "$$y = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{-e^{-x}}{(1 + e^{-x})^2} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "2. ReLU activation\n",
    "\n",
    "$$y = ReLU(x) = max(0, x)$$\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x\\geq 0\\\\\n",
    "      0 & x\\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.$$\n",
    "\n",
    "3. Tanh activation\n",
    "$$y = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "$$\\frac{dy}{dx} = \\frac{4}{(e^x + e^{-x})^2} = 1 - (tanh(x))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interested-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(AutoDiffFunction):\n",
    "    \"\"\" \n",
    "    Represents the Sigmoid Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = 1/(1 + np.exp(-x))\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": y*(1-y)}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]      \n",
    "\n",
    "\n",
    "class ReLU(AutoDiffFunction):\n",
    "    \"\"\" \n",
    "    Represents the RelU Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = np.where(x>0.0, 1.0, 0.0)\n",
    "\n",
    "        return x * self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        return {\"x\": self.saved_for_backward}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]\n",
    "    \n",
    "class Tanh(AutoDiffFunction):\n",
    "    \"\"\" \n",
    "    Represents the Tanh Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": 1 - y**2}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-taiwan",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "For this assignment, we only use fully connected OR Dense layers where each input neuron is connected to each output neuron, along with a bias unit. Below is a representation of a fully connected layer.\n",
    "\n",
    "![Representation of a fully connected layer](FullyConnectedLayer.png)\n",
    "\n",
    "The equation for such a layer is simply\n",
    "\n",
    "$$y = FullyConnected(x) = wx + b$$\n",
    "\n",
    "$$\\frac{dy}{dw} = x^T \\quad\\frac{dy}{dx} = w^T  \\quad\\frac{dy}{db} = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "announced-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Layer):\n",
    "    def __init__(self, in_dim, out_dim, weight_decay=None, init_method=\"random\") -> None:\n",
    "        super().__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_method = init_method\n",
    "        self.initialize_weights(in_dim, out_dim)\n",
    "\n",
    "    def initialize_weights(self, in_dim, out_dim):\n",
    "        \n",
    "        if self.init_method == \"random\":\n",
    "            scaling_factor = 1/np.sqrt(in_dim)\n",
    "            self.weights[\"w\"] = np.random.randn(in_dim, out_dim) * scaling_factor\n",
    "            self.weights[\"b\"] = np.random.randn(1, out_dim) * scaling_factor\n",
    "        elif self.init_method == \"xavier\":\n",
    "            lim = np.sqrt(6 / (in_dim + out_dim))\n",
    "            self.weights[\"w\"] = np.random.uniform(low=-lim, high=lim, size=(in_dim, out_dim))\n",
    "            self.weights[\"b\"] = np.random.uniform(low=-lim, high=lim, size=(1, out_dim))\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        \n",
    "        gradients = {}\n",
    "\n",
    "        # y = x * w + b        \n",
    "        # we compute gradients wrt w and x \n",
    "        # gradient wrt b is not required explicitly since we know that it's value is 1\n",
    "        gradients[\"w\"] = self.saved_for_backward[\"x\"].T\n",
    "        gradients[\"x\"] = self.weights[\"w\"].T\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x @ self.weights[\"w\"] + self.weights[\"b\"]\n",
    "        self.saved_for_backward[\"x\"] = x\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \n",
    "        # calculating gradients wrt input to pass on to previous layer for backprop\n",
    "        dx = dy @ self.grad[\"x\"]\n",
    "        \n",
    "        # calculating gradients wrt weights\n",
    "        dw = self.grad[\"w\"] @ dy\n",
    "        db = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "        # accomodating for weight_decay / regularization\n",
    "        if self.weight_decay:\n",
    "            dw = dw + 2 * self.weight_decay * self.weights[\"w\"]\n",
    "            db = db + 2 * self.weight_decay * self.weights[\"b\"]\n",
    "\n",
    "        self.absolute_gradients = {\"w\": dw, \"b\": db}\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.optimizer.step(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-penny",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The loss function dictates how good the output of the neural network is. Since we use MNIST dataset, our job is classification and hence we use the Categorical CrossEntropy loss function. We also have created the Mean Squared Error loss function to check how it performs for a classification task for which it's not meant. \n",
    "\n",
    "#### 1) CrossEntropy Loss\n",
    "\n",
    "$$L(p, y) = \\Sigma_{i=1}^{N} \\Sigma_{k=1}^{K} y_{ik} \\log p_{ik}$$ \n",
    "\n",
    "where $$y_{ik} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x \\in class-k\\\\\n",
    "      0 & else \\\\\n",
    "\\end{array} \n",
    "\\right.$$\n",
    "\n",
    "$p_{ik} =$ probability that $i^{th}$ sample falls in $k^{th}$ class\n",
    "\n",
    "In our implementation, the given loss function is applied along with the activation function for the last layer i.e. Softmax activation. It's formula is given by the following equation\n",
    "\n",
    "$ f: [x_1, x_2, ... x_k] \\rightarrow [p_1, p_2, ... p_k]$ such that $p_i = \\frac{e^{x_i}}{\\Sigma_{k=1}^{K} e^{x_i}}$\n",
    "\n",
    "Now, to find the derivative of loss w.r.t input we have apply the chain rule. Let $p(x)$ represent the softmax activation and $L$ represent the loss. Then the expression turns out to be\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L(p, y)}{\\partial p} \\frac{\\partial p(x)}{\\partial x} = p - y$$\n",
    "\n",
    "#### 2) Mean Squared Loss OR L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFromLogits(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.n_classes = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        v = np.exp(x)\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "\n",
    "    def encode(self, y): \n",
    "        encoded_y = np.zeros(shape=(len(y), self.n_classes))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "\n",
    "        return encoded_y\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "         \n",
    "        probabilities = self.softmax(y_pred)\n",
    "        y_true_encoded = self.encode(y_true)\n",
    "\n",
    "        loss_value = np.mean(np.sum(- y_true_encoded * np.log(probabilities), axis=1))\n",
    "\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def compute_grad(self, y_pred, y_true):\n",
    "\n",
    "        return {\"x\": self.saved_for_backward[\"probabilities\"] - self.saved_for_backward[\"y_true\"]}        \n",
    "\n",
    "\n",
    "class MSELossFromLogits(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.n_classes = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        v = np.exp(x)\n",
    "\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "\n",
    "    def encode(self, y): \n",
    "        encoded_y = np.zeros(shape=(len(y), self.n_classes))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "\n",
    "        return encoded_y\n",
    "    \n",
    "    @staticmethod\n",
    "    def indicator(i, j):\n",
    "        ind = {True: 1, False: 0}\n",
    "        return ind[i==j]\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "         \n",
    "        probabilities = self.softmax(y_pred)\n",
    "        y_true_encoded = self.encode(y_true)\n",
    "\n",
    "        loss_value = np.mean(np.sum((probabilities - y_true_encoded)**2, axis=1))\n",
    "\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def compute_grad(self, y_pred, y_true):\n",
    "\n",
    "        probs = self.saved_for_backward[\"probabilities\"]\n",
    "        labels = self.saved_for_backward[\"y_true\"]\n",
    "        grad = np.zeros(shape=(len(y_true), self.n_classes))\n",
    "        \n",
    "        for point_counter in range(len(y_true)):\n",
    "            res = 0\n",
    "            for i in range(self.n_classes):\n",
    "                for j in range(self.n_classes):\n",
    "                    \n",
    "                    res = probs[point_counter, j] * (probs[point_counter, j] - labels[point_counter, j]) * (self.indicator(i,j) - probs[point_counter, i])\n",
    "                \n",
    "                grad[point_counter, i] = res\n",
    "        \n",
    "        return {\"x\": grad}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-compression",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Optimizers basically denote how to make use of the gradients achieved through backpropogation to update the weights of the model. Based on the question, we have created the following 6 optimizers.\n",
    "\n",
    "1) sgd<br>\n",
    "2) momentum based gradient descent<br>\n",
    "3) nesterov accelerated gradient descent<br>\n",
    "4) rmsprop<br>\n",
    "5) adam<br>\n",
    "6) nadam<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleasant-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, layer):\n",
    "\n",
    "        for weight_name, _ in layer.weights.items():\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - self.lr * layer.absolute_gradients[weight_name]\n",
    "\n",
    "class Nadam(Optimizer):\n",
    "    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self, layer):\n",
    "        \n",
    "        # we have 2 parameters to remember m(t) and v(t) for all weights in the layer\n",
    "        if self.remember == {}:\n",
    "            for weight_name, weight in layer.weights.items():\n",
    "                self.remember[weight_name] = {}\n",
    "                self.remember[weight_name][\"v\"] = np.zeros_like(weight)\n",
    "                self.remember[weight_name][\"m\"] = np.zeros_like(weight)\n",
    "\n",
    "        for weight_name, weight in layer.weights.items():\n",
    "            \n",
    "            self.remember[weight_name][\"m\"] = self.beta_1 * self.remember[weight_name][\"m\"] + \\\n",
    "                                                (1 -self.beta_1) * layer.absolute_gradients[weight_name]\n",
    "\n",
    "            self.remember[weight_name][\"v\"] = self.beta_2 * self.remember[weight_name][\"v\"] + \\\n",
    "                                                (1 - self.beta_2) * layer.absolute_gradients[weight_name]**2\n",
    "\n",
    "            # bias correction step \n",
    "            m_hat = self.remember[weight_name][\"m\"]/(1 - self.beta_1 ** self.t)\n",
    "            v_hat = self.remember[weight_name][\"v\"]/(1 - self.beta_2 ** self.t)\n",
    "\n",
    "            d = self.lr / (np.sqrt(v_hat) + self.epsilon) * (self.beta_1*m_hat + (1-self.beta_1)/\n",
    "                                                (1-self.beta_1 ** self.t) * layer.absolute_gradients[weight_name]) \n",
    "\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - d\n",
    "\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-cleveland",
   "metadata": {},
   "source": [
    "## Framework for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enclosed-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers = layers\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.forward(*args, **kwds)\n",
    "\n",
    "    def compile(self, loss, optimizer):\n",
    "        self.loss = loss\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.optimizer = deepcopy(optimizer)\n",
    "\n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        return self.loss(y_pred, y_true)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        gradient = self.loss.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def update_weights(self):\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.update_weights()\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_score(y_pred, y_true):\n",
    "\n",
    "        pred_labels = np.argmax(y_pred, axis=1)\n",
    "        return np.sum(pred_labels == y_true) / len(y_true)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_batches(X, y, batch_size=32):\n",
    "        batches = []\n",
    "\n",
    "        for i in range(len(y) // batch_size):\n",
    "            start_idx = batch_size * i\n",
    "            end_idx = batch_size * (i + 1)\n",
    "\n",
    "            batches.append([X[start_idx: end_idx], y[start_idx: end_idx]])\n",
    "\n",
    "        # take care of the last batch which might have batch_size less than the specified one\n",
    "        if len(y) % batch_size != 0:\n",
    "            batches.append([X[end_idx:], y[end_idx:]])\n",
    "\n",
    "        return batches\n",
    "\n",
    "\n",
    "    def fit(self, X, y, batch_size=32, epochs=10):\n",
    "\n",
    "        # calculate number of classes to pass to the loss function\n",
    "        self.loss.n_classes = len(np.unique(y))\n",
    "\n",
    "        batches = self.create_batches(X, y, batch_size=batch_size)\n",
    "        num_batches = len(batches)\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "\n",
    "            for X, y in batches:\n",
    "\n",
    "                preds = self(X)\n",
    "                total_loss += self.loss(preds, y)\n",
    "                total_accuracy += self.accuracy_score(preds, y)\n",
    "\n",
    "                _ = self.backward()\n",
    "                self.update_weights()\n",
    "\n",
    "            loss_per_epoch = total_loss / num_batches\n",
    "            accuracy = total_accuracy / num_batches\n",
    "\n",
    "            print(f\"Epoch: {epoch} Train Loss: {loss_per_epoch} Train Accuracy: {accuracy}\")\n",
    "\n",
    "            self.history.append({\"Epoch\" : epoch, \n",
    "                                    \"Train Loss\": loss_per_epoch,\n",
    "                                    \"Train Accuracy\": accuracy})\n",
    "\n",
    "        print(\"\\nModel trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-graduation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
