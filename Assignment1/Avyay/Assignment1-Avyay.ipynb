{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from templates import AutoDiffFunction, Layer, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(AutoDiffFunction):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = 1/(1 + np.exp(-x))\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": y*(1-y)}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]      \n",
    "\n",
    "\n",
    "class RelU(AutoDiffFunction):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = np.where(x>0.0, 1.0, 0.0)\n",
    "\n",
    "        return x * self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        return {\"x\": self.saved_for_backward}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]\n",
    "    \n",
    "class Tanh(AutoDiffFunction):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": 1 - y**2}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]\n",
    "    \n",
    "class Softmax(AutoDiffFunction):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = np.exp(x)\n",
    "        self.saved_for_backward = v\n",
    "\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Layer):\n",
    "    def __init__(self, in_dim, out_dim, weight_decay=None, init_method=\"random\") -> None:\n",
    "        super().__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_method = init_method\n",
    "        self.initialize_weights(in_dim, out_dim)\n",
    "\n",
    "    def initialize_weights(self, in_dim, out_dim):\n",
    "        \n",
    "        if self.init_method == \"random\":\n",
    "            scaling_factor = 1/np.sqrt(in_dim)\n",
    "            self.weights[\"w\"] = np.random.randn(in_dim, out_dim) * scaling_factor\n",
    "            self.weights[\"b\"] = np.random.randn(1, out_dim) * scaling_factor\n",
    "        elif self.init_method == \"xavier\":\n",
    "            lim = np.sqrt(6 / (in_dim + out_dim))\n",
    "            self.weights[\"w\"] = np.random.uniform(low=-lim, high=lim, size=(in_dim, out_dim))\n",
    "            self.weights[\"b\"] = np.random.uniform(low=-lim, high=lim, size=(1, out_dim))\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        \n",
    "        gradients = {}\n",
    "\n",
    "        # y = x * w + b        \n",
    "        # we compute gradients wrt w and x \n",
    "        # gradient wrt b is not required explicitly since we know that it's value is 1\n",
    "        gradients[\"w\"] = self.saved_for_backward[\"x\"].T\n",
    "        gradients[\"x\"] = self.weights[\"w\"].T\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x @ self.weights[\"w\"] + self.weights[\"b\"]\n",
    "        self.saved_for_backward[\"x\"] = x\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \n",
    "        # calculating gradients wrt input to pass on to previous layer for backprop\n",
    "        dx = dy @ self.grad[\"x\"]\n",
    "        \n",
    "        # calculating gradients wrt weights\n",
    "        dw = self.grad[\"w\"] @ dy\n",
    "        db = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "        # accomodating for weight_decay / regularization\n",
    "        if self.weight_decay:\n",
    "            dw = dw + 2 * self.weight_decay * self.weights[\"w\"]\n",
    "            db = db + 2 * self.weight_decay * self.weights[\"b\"]\n",
    "\n",
    "        self.absolute_gradients = {\"w\": dw, \"b\": db}\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.optimizer.step(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFromLogits(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.n_classes = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        v = np.exp(x)\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "\n",
    "    def encode(self, y): \n",
    "        encoded_y = np.zeros(shape=(len(y), self.n_classes))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "\n",
    "        return encoded_y\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "         \n",
    "        probabilities = self.softmax(y_pred)\n",
    "        y_true_encoded = self.encode(y_true)\n",
    "\n",
    "        loss_value = np.mean(np.sum(- y_true_encoded * np.log(probabilities), axis=1))\n",
    "\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def compute_grad(self, y_pred, y_true):\n",
    "\n",
    "        return {\"x\": self.saved_for_backward[\"probabilities\"] - self.saved_for_backward[\"y_true\"]}\n",
    "    \n",
    "class MSELossFromLogits(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.n_classes = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        maxes_vec = np.expand_dims(np.amax(x,axis=1), axis=1)\n",
    "        maxes_arr = np.tile(maxes_vec, (1,x.shape[1]))\n",
    "        v = np.exp(x - maxes_arr)\n",
    "\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "\n",
    "    def encode(self, y): \n",
    "        encoded_y = np.zeros(shape=(len(y), self.n_classes))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "\n",
    "        return encoded_y\n",
    "    \n",
    "    @staticmethod\n",
    "    def indicator(i, j):\n",
    "        ind = {True: 1, False: 0}\n",
    "        return ind[i==j]\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "         \n",
    "        probabilities = self.softmax(y_pred)\n",
    "        y_true_encoded = self.encode(y_true)\n",
    "\n",
    "        loss_value = np.mean(np.sum((probabilities - y_true_encoded)**2, axis=1))\n",
    "\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def compute_grad(self, y_pred, y_true):\n",
    "\n",
    "        probs = self.saved_for_backward[\"probabilities\"]\n",
    "        labels = self.saved_for_backward[\"y_true\"]\n",
    "        grad = np.zeros(shape=(len(y_true), self.n_classes))\n",
    "        \n",
    "        for point_counter in range(len(y_true)):\n",
    "            res = 0\n",
    "            for i in range(self.n_classes):\n",
    "                for j in range(self.n_classes):\n",
    "                    \n",
    "                    res = probs[point_counter, j] * (probs[point_counter, j] - labels[point_counter, j]) * (self.indicator(i,j) - probs[point_counter, i])\n",
    "                \n",
    "                grad[point_counter, i] = res\n",
    "        \n",
    "        return {\"x\": grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]\n",
      " [0.04201007 0.1141952  0.84379473]]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([[1,2,3],[4,5,7]])\n",
    "v = np.exp(arr1)\n",
    "print(v / np.sum(v, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=1e-2):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, layer):\n",
    "\n",
    "        for weight_name, _ in layer.weights.items():\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - self.lr * layer.absolute_gradients[weight_name]\n",
    "            \n",
    "class Momentum(Optimizer):\n",
    "    def __init__(self, lr=1e-3, gamma=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def step(self, layer):\n",
    "        \n",
    "        #Initialise update history\n",
    "        if self.remember == {}:\n",
    "            for weight_name, weight in layer.weights.items():\n",
    "                self.remember[weight_name] = {}\n",
    "                self.remember[weight_name][\"v\"] = np.zeros_like(weight)\n",
    "        \n",
    "        #Momentum update rule\n",
    "        for weight_name, weight in layer.weights.items():\n",
    "            self.remember[weight_name][\"v\"] = self.gamma * self.remember[weight_name][\"v\"] + \\\n",
    "                                                self.lr * layer.absolute_gradients[weight_name]\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - self.remember[weight_name][\"v\"]\n",
    "\n",
    "\"\"\"\n",
    "class NAG(Optimizer):\n",
    "    def __init__(self, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def step(self, layer):\n",
    "        \n",
    "        #Initialise update history\n",
    "        if self.remember == {}:\n",
    "            self.remember[weight_name] = {}\n",
    "            self.remember[weight_name][\"v\"] = np.zeros_like(weight)\n",
    "            self.remember[weight_name][\"look_ahead\"] = np.zeros_like(weight)\n",
    "        \n",
    "        #NAG update rule\n",
    "        for weight_name, weight in layers.weights.items():\n",
    "            self.remember[weight_name][\"look_ahead\"] = layer.weights[weight_name] - self.gamma * self.remember[weight_name][\"v\"]\n",
    "            #self.remember[weight_name][\"v\"] = \n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - self.remember[weight_name][\"v\"]\n",
    "            \n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, lr=1e-3, beta=0.9, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def step(self, layer):\n",
    "        \n",
    "        #Initialise update history\n",
    "        if self.remember == {}:\n",
    "            for weight_name, weight in layer.weights.items():\n",
    "                self.remember[weight_name] = {}\n",
    "                self.remember[weight_name][\"v\"] = np.zeros_like(weight)\n",
    "        \n",
    "        #RMSprop update rule\n",
    "        for weight_name, weight in layer.weights.items():\n",
    "            self.remember[weight_name][\"v\"] = self.beta * self.remember[weight_name][\"v\"] + \\\n",
    "                                                (1 - self.beta) * (layer.absolute_gradients[weight_name] ** 2)\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - (self.lr / (np.sqrt(self.remember[weight_name][\"v\"] + \\\n",
    "                                                self.epsilon))) * layer.weights[weight_name]\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 1\n",
    "        \n",
    "    def step(self, layer):\n",
    "        \n",
    "        #Initialise update history\n",
    "        if self.remember == {}:\n",
    "            for weight_name, weight in layer.weights.items():\n",
    "                self.remember[weight_name] = {}\n",
    "                self.remember[weight_name][\"v\"] = np.zeros_like(weight)\n",
    "                self.remember[weight_name][\"m\"] = np.zeros_like(weight)\n",
    "        \n",
    "        #Adam update rule\n",
    "        for weight_name, weight in layer.weights.items():\n",
    "            \n",
    "            #Update m_t and v_t\n",
    "            self.remember[weight_name][\"m\"] = self.beta_1 * self.remember[weight_name][\"m\"] + \\\n",
    "                                                (1 -self.beta_1) * layer.absolute_gradients[weight_name]\n",
    "            \n",
    "            self.remember[weight_name][\"v\"] = self.beta_2 * self.remember[weight_name][\"v\"] + \\\n",
    "                                                (1 - self.beta_2) * (layer.absolute_gradients[weight_name]**2)\n",
    "            \n",
    "            #Bias correction\n",
    "            m_hat = self.remember[weight_name][\"m\"]/(1 - self.beta_1 ** self.t)\n",
    "            v_hat = self.remember[weight_name][\"v\"]/(1 - self.beta_2 ** self.t)\n",
    "            \n",
    "            #Update parameters\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - (self.lr / (np.sqrt(v_hat + self.epsilon))) * m_hat\n",
    "            \n",
    "        self.t += 1\n",
    "            \n",
    "class Nadam(Optimizer):\n",
    "    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self, layer):\n",
    "        \n",
    "        # we have 2 parameters to remember m(t) and v(t) for all weights in the layer\n",
    "        if self.remember == {}:\n",
    "            for weight_name, weight in layer.weights.items():\n",
    "                self.remember[weight_name] = {}\n",
    "                self.remember[weight_name][\"v\"] = np.zeros_like(weight)\n",
    "                self.remember[weight_name][\"m\"] = np.zeros_like(weight)\n",
    "\n",
    "        for weight_name, weight in layer.weights.items():\n",
    "            \n",
    "            self.remember[weight_name][\"m\"] = self.beta_1 * self.remember[weight_name][\"m\"] + \\\n",
    "                                                (1 -self.beta_1) * layer.absolute_gradients[weight_name]\n",
    "\n",
    "            self.remember[weight_name][\"v\"] = self.beta_2 * self.remember[weight_name][\"v\"] + \\\n",
    "                                                (1 - self.beta_2) * layer.absolute_gradients[weight_name]**2\n",
    "\n",
    "            # bias correction step \n",
    "            m_hat = self.remember[weight_name][\"m\"]/(1 - self.beta_1 ** self.t)\n",
    "            v_hat = self.remember[weight_name][\"v\"]/(1 - self.beta_2 ** self.t)\n",
    "\n",
    "            d = self.lr / (np.sqrt(v_hat) + self.epsilon) * (self.beta_1*m_hat + (1-self.beta_1)/\n",
    "                                                (1-self.beta_1 ** self.t) * layer.absolute_gradients[weight_name]) \n",
    "\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - d\n",
    "\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers = layers\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.forward(*args, **kwds)\n",
    "\n",
    "    def compile(self, loss, optimizer):\n",
    "        self.loss = loss\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.optimizer = deepcopy(optimizer)\n",
    "\n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        return self.loss(y_pred, y_true)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        gradient = self.loss.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def update_weights(self):\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.update_weights()\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_score(y_pred, y_true):\n",
    "\n",
    "        pred_labels = np.argmax(y_pred, axis=1)\n",
    "        return np.sum(pred_labels == y_true) / len(y_true)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_batches(X, y, batch_size=32):\n",
    "        batches = []\n",
    "\n",
    "        for i in range(len(y) // batch_size):\n",
    "            start_idx = batch_size * i\n",
    "            end_idx = batch_size * (i + 1)\n",
    "\n",
    "            batches.append([X[start_idx: end_idx], y[start_idx: end_idx]])\n",
    "\n",
    "        # take care of the last batch which might have batch_size less than the specified one\n",
    "        if len(y) % batch_size != 0:\n",
    "            batches.append([X[end_idx:], y[end_idx:]])\n",
    "\n",
    "        return batches\n",
    "\n",
    "\n",
    "    def fit(self, X, y, batch_size=32, epochs=10):\n",
    "\n",
    "        # calculate number of classes to pass to the loss function\n",
    "        self.loss.n_classes = len(np.unique(y))\n",
    "\n",
    "        batches = self.create_batches(X, y, batch_size=batch_size)\n",
    "        num_batches = len(batches)\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "\n",
    "            for X, y in batches:\n",
    "\n",
    "                preds = self(X)\n",
    "                total_loss += self.loss(preds, y)\n",
    "                total_accuracy += self.accuracy_score(preds, y)\n",
    "\n",
    "                _ = self.backward()\n",
    "                self.update_weights()\n",
    "\n",
    "            loss_per_epoch = total_loss / num_batches\n",
    "            accuracy = total_accuracy / num_batches\n",
    "\n",
    "            print(f\"Epoch: {epoch} Train Loss: {loss_per_epoch} Train Accuracy: {accuracy}\")\n",
    "\n",
    "            self.history.append({\"Epoch\" : epoch, \n",
    "                                    \"Train Loss\": loss_per_epoch,\n",
    "                                    \"Train Accuracy\": accuracy})\n",
    "\n",
    "        print(\"\\nModel trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, batches, loss, optimizer, epochs=10):\n",
    "\n",
    "    training_stats = []\n",
    "    num_batches = len(batches)\n",
    " \n",
    "    #loss = CrossEntropyLossFromLogits()\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        for X, y in batches:\n",
    "            \n",
    "            preds = model(X)\n",
    "            total_loss += model.loss(preds, y)\n",
    "            total_accuracy += accuracy_score(preds, y)\n",
    "\n",
    "            _ = model.backward()\n",
    "            model.update_weights()\n",
    "\n",
    "        loss_per_epoch = total_loss / num_batches\n",
    "        accuracy = total_accuracy / num_batches\n",
    "\n",
    "        print(f\"Epoch: {epoch} Train Loss: {loss_per_epoch} Train Accuracy: {accuracy}\")\n",
    "\n",
    "        training_stats.append({\"Epoch\" : epoch, \n",
    "                                \"Train Loss\": loss_per_epoch,\n",
    "                                \"Train Accuracy\": accuracy})\n",
    "\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Dummy Dataset to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 1.296151012926381 Train Accuracy: 0.359375\n",
      "Epoch: 2 Train Loss: 1.2470655474706105 Train Accuracy: 0.375\n",
      "Epoch: 3 Train Loss: 1.205219470817868 Train Accuracy: 0.3802083333333333\n",
      "Epoch: 4 Train Loss: 1.167268092152686 Train Accuracy: 0.4010416666666667\n",
      "Epoch: 5 Train Loss: 1.1325294011516405 Train Accuracy: 0.4166666666666667\n",
      "Epoch: 6 Train Loss: 1.1005629934833294 Train Accuracy: 0.4375\n",
      "Epoch: 7 Train Loss: 1.0709038526064185 Train Accuracy: 0.4635416666666667\n",
      "Epoch: 8 Train Loss: 1.0431167270016186 Train Accuracy: 0.4895833333333333\n",
      "Epoch: 9 Train Loss: 1.0167965522501972 Train Accuracy: 0.515625\n",
      "Epoch: 10 Train Loss: 0.9917978042697341 Train Accuracy: 0.53125\n",
      "Epoch: 11 Train Loss: 0.9679441755551066 Train Accuracy: 0.5520833333333334\n",
      "Epoch: 12 Train Loss: 0.9450497025878044 Train Accuracy: 0.5625\n",
      "Epoch: 13 Train Loss: 0.9230976850093287 Train Accuracy: 0.5885416666666666\n",
      "Epoch: 14 Train Loss: 0.901965242276925 Train Accuracy: 0.59375\n",
      "Epoch: 15 Train Loss: 0.8816329718507783 Train Accuracy: 0.6197916666666666\n",
      "Epoch: 16 Train Loss: 0.8620122027201748 Train Accuracy: 0.6302083333333334\n",
      "Epoch: 17 Train Loss: 0.842976532528577 Train Accuracy: 0.6458333333333334\n",
      "Epoch: 18 Train Loss: 0.8244928756919774 Train Accuracy: 0.6614583333333334\n",
      "Epoch: 19 Train Loss: 0.8064417078237255 Train Accuracy: 0.671875\n",
      "Epoch: 20 Train Loss: 0.7888387356212568 Train Accuracy: 0.671875\n",
      "Epoch: 21 Train Loss: 0.7717289384865277 Train Accuracy: 0.6875\n",
      "Epoch: 22 Train Loss: 0.7550581632285601 Train Accuracy: 0.6979166666666666\n",
      "Epoch: 23 Train Loss: 0.7387595215446231 Train Accuracy: 0.6979166666666666\n",
      "Epoch: 24 Train Loss: 0.7228278893769943 Train Accuracy: 0.71875\n",
      "Epoch: 25 Train Loss: 0.7072342020979719 Train Accuracy: 0.7291666666666666\n",
      "Epoch: 26 Train Loss: 0.6920458285424131 Train Accuracy: 0.7447916666666666\n",
      "Epoch: 27 Train Loss: 0.6773323868437057 Train Accuracy: 0.7604166666666666\n",
      "Epoch: 28 Train Loss: 0.663114038006256 Train Accuracy: 0.7708333333333334\n",
      "Epoch: 29 Train Loss: 0.6492966360614552 Train Accuracy: 0.7708333333333334\n",
      "Epoch: 30 Train Loss: 0.6358762222888945 Train Accuracy: 0.7760416666666666\n",
      "Epoch: 31 Train Loss: 0.6228419267620005 Train Accuracy: 0.78125\n",
      "Epoch: 32 Train Loss: 0.6102229788428423 Train Accuracy: 0.7864583333333334\n",
      "Epoch: 33 Train Loss: 0.5980040607897482 Train Accuracy: 0.7916666666666666\n",
      "Epoch: 34 Train Loss: 0.5861704945110723 Train Accuracy: 0.796875\n",
      "Epoch: 35 Train Loss: 0.5747699846019295 Train Accuracy: 0.8333333333333334\n",
      "Epoch: 36 Train Loss: 0.5638094917873578 Train Accuracy: 0.8385416666666666\n",
      "Epoch: 37 Train Loss: 0.553178748441436 Train Accuracy: 0.8489583333333334\n",
      "Epoch: 38 Train Loss: 0.5428357115645491 Train Accuracy: 0.8489583333333334\n",
      "Epoch: 39 Train Loss: 0.5327783152799259 Train Accuracy: 0.84375\n",
      "Epoch: 40 Train Loss: 0.5229798305610548 Train Accuracy: 0.8541666666666666\n",
      "Epoch: 41 Train Loss: 0.5135008377634595 Train Accuracy: 0.8541666666666666\n",
      "Epoch: 42 Train Loss: 0.5043563567084499 Train Accuracy: 0.8541666666666666\n",
      "Epoch: 43 Train Loss: 0.49545759771783665 Train Accuracy: 0.859375\n",
      "Epoch: 44 Train Loss: 0.4868197049824892 Train Accuracy: 0.859375\n",
      "Epoch: 45 Train Loss: 0.47847681425406247 Train Accuracy: 0.859375\n",
      "Epoch: 46 Train Loss: 0.47041783697949874 Train Accuracy: 0.8645833333333334\n",
      "Epoch: 47 Train Loss: 0.46259404288796385 Train Accuracy: 0.8645833333333334\n",
      "Epoch: 48 Train Loss: 0.45498697100837426 Train Accuracy: 0.8697916666666666\n",
      "Epoch: 49 Train Loss: 0.447639895383426 Train Accuracy: 0.875\n",
      "Epoch: 50 Train Loss: 0.4405409994136271 Train Accuracy: 0.8802083333333334\n",
      "\n",
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "## creating a dummy dataset to test out stuff ##\n",
    "\n",
    "X, y = make_classification(n_samples=32*6, n_features=20, n_informative=15, n_classes=3)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Initializing the model and setting up loss and optimizer\n",
    "model = NeuralNet([FC(20, 32), RelU(), FC(32, 3)])\n",
    "optimizer = Adam()\n",
    "loss = CrossEntropyLossFromLogits()\n",
    "\n",
    "model.compile(loss, optimizer)\n",
    "model.fit(X, y, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(dataset):\n",
    "    flattened_data = np.zeros((dataset.shape[0], dataset.shape[1]*dataset.shape[2]))\n",
    "    for i in range(dataset.shape[0]):\n",
    "        flattened_data[i] = dataset[i].reshape((dataset.shape[1]*dataset.shape[2],))\n",
    "    return flattened_data\n",
    "\n",
    "def normalize_image(dataset):\n",
    "    return dataset/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.54358026484769 Train Accuracy: 0.8041833333333334\n",
      "Epoch: 2 Train Loss: 0.4013119717257807 Train Accuracy: 0.8544333333333334\n",
      "Epoch: 3 Train Loss: 0.36612204408018384 Train Accuracy: 0.8660666666666667\n",
      "Epoch: 4 Train Loss: 0.34396696378716873 Train Accuracy: 0.8736333333333334\n",
      "Epoch: 5 Train Loss: 0.33000630347224313 Train Accuracy: 0.8779166666666667\n",
      "\n",
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "train_data = normalize_image(flatten_data(trainX))\n",
    "test_data = normalize_image(flatten_data(testX))\n",
    "\n",
    "model = NeuralNet([FC(784, 32), RelU(), FC(32, 32), RelU(), FC(32, 32), RelU(), FC(32, 32), RelU(), FC(32, 10)])\n",
    "optimizer = Adam()\n",
    "loss = CrossEntropyLossFromLogits()\n",
    "\n",
    "model.compile(loss, optimizer)\n",
    "model.fit(train_data, trainY, batch_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
